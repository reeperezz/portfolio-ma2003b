{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages (0.14.5)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages (from statsmodels) (2.2.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages (from statsmodels) (1.15.2)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages (from statsmodels) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages (from statsmodels) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install missing package (required when ModuleNotFoundError occurs)\n",
    "%pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from factor_analyzer import FactorAnalyzer, Rotator\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from dash import Dash, dcc, html, Input, Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1: Data Exploration and Suitability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'customer_satisfaction_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This cell loads the customer satisfaction dataset, displays its structure, and shows the data types of each column to verify proper formatting before analysis.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustomer_satisfaction_data.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mESTRUCTURA DEL DATASET:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFilas, Columnas:\u001b[39m\u001b[33m\"\u001b[39m, data.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/sklearn-env/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'customer_satisfaction_data.csv'"
     ]
    }
   ],
   "source": [
    "# This cell loads the customer satisfaction dataset, displays its structure, and shows the data types of each column to verify proper formatting before analysis.\n",
    "\n",
    "data = pd.read_csv(\"customer_satisfaction_data.csv\")\n",
    "print('ESTRUCTURA DEL DATASET:')\n",
    "print(\"Filas, Columnas:\", data.shape)\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we organizes variables into business dimensions and outcome measures, removes missing values, and prepares the dataset for factor analysis and modeling.\n",
    "\n",
    "val = data.isna().sum().sum()\n",
    "pct = 100 * val / (data.shape[0] * data.shape[1])\n",
    "print(f\"\\nCeldas faltantes totales: {val} ({pct:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define business-related variable groups and outcome metrics, removes missing values, and prepares the data subsets for further factor analysis.\n",
    " \n",
    "items = [\n",
    "    # Technical Excellence & Innovation\n",
    "    \"technical_expertise\",\n",
    "    \"problem_solving\",\n",
    "    \"innovation_solutions\",\n",
    "    \"technical_documentation\",\n",
    "    \"system_integration\",\n",
    "\n",
    "    # Relationship Management & Communication\n",
    "    \"account_manager_responsive\",\n",
    "    \"executive_access\",\n",
    "    \"trust_reliability\",\n",
    "    \"long_term_partnership\",\n",
    "    \"communication_clarity\",\n",
    "\n",
    "    # Project Delivery & Quality\n",
    "    \"project_management\",\n",
    "    \"timeline_adherence\",\n",
    "    \"budget_control\",\n",
    "    \"quality_deliverables\",\n",
    "    \"change_management\",\n",
    "\n",
    "    # Value & Financial Transparency\n",
    "    \"cost_transparency\",\n",
    "    \"value_for_money\",\n",
    "    \"roi_demonstration\",\n",
    "    \"competitive_pricing\",\n",
    "    \"billing_accuracy\",\n",
    "\n",
    "    # Support & Service Excellence\n",
    "    \"support_responsiveness\",\n",
    "    \"training_quality\",\n",
    "    \"documentation_help\"\n",
    "]\n",
    "outcome_cols = [\n",
    "    \"overall_satisfaction\",\n",
    "    \"nps_score\",\n",
    "    \"renewal_likelihood\",\n",
    "    \"revenue_growth_pct\",\n",
    "    \"referrals_generated\"\n",
    "]\n",
    "\n",
    "\n",
    "data = data.dropna()\n",
    "data_obj = data[outcome_cols]\n",
    "data = data[items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates basic descriptive statistics (mean, std, min, max, quartiles) to understand the distribution and scale of the survey variables before analysis.\n",
    "\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: Variability and Consistency\n",
    "\n",
    "The variability analysis (standard deviation per variable) shows how consistent or dispersed customer opinions are across items.\n",
    "\n",
    "The most variable items (often related to pricing and value perception) suggest mixed customer experiences.\n",
    "\n",
    "Meanwhile, items with low variability, mainly in technical and service excellence, reflect strong internal agreement among respondents.\n",
    "\n",
    "This analysis complements the satisfaction ranking, giving a clearer view of which dimensions are stable strengths and which require targeted improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean satisfaction score for each variable\n",
    "# and sort them in descending order (from highest to lowest satisfaction)\n",
    "mean_scores = data.mean().sort_values(ascending=False)\n",
    "\n",
    "# Display the Top 5 variables with the highest average satisfaction\n",
    "print(\"Top 5 variables with highest satisfaction:\\n\")\n",
    "print(mean_scores.head(5).round(3))\n",
    "\n",
    "# Display the Bottom 5 variables with the lowest average satisfaction\n",
    "print(\"\\nBottom 5 variables with lowest satisfaction:\\n\")\n",
    "print(mean_scores.tail(5).round(3))\n",
    "\n",
    "# Variability of responses\n",
    "std_scores = data.std().sort_values(ascending=False)\n",
    "\n",
    "# Display the 5 variables with the highest variability (more dispersed opinions)\n",
    "print(\"\\nVariables with highest variability (most dispersed opinions):\\n\")\n",
    "print(std_scores.head(5).round(3))\n",
    "\n",
    "# Display the 5 variables with the lowest variability (more consistent opinions)\n",
    "\n",
    "print(\"\\nVariables with lowest variability (most consistent opinions):\\n\")\n",
    "print(std_scores.tail(5).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top five variables show the strongest satisfaction levels, mainly associated with \n",
    "**technical expertise, innovation solutions, and problem-solving**, suggesting consistent \n",
    "strengths in technical performance.  \n",
    "\n",
    "In contrast, the bottom five items (including **cost transparency** and **value for money** )\n",
    "highlight opportunities to improve customer perception regarding pricing and financial fairness.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell calculates and visualizes the correlation matrix between variables to identify relationships and potential factor groupings for the analysis.\n",
    "\n",
    "corr = data.corr()\n",
    "\n",
    "fig_corr = px.imshow(corr, text_auto=True, title='Matriz de correlaciones')\n",
    "fig_corr.update_layout(width=600, height=600)\n",
    "fig_corr.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies the strongest correlations between variables (excluding the diagonal) to detect potential latent patterns and relationships relevant for factor extraction.\n",
    "\n",
    "# Calculate the correlation matrix only for numeric variables\n",
    "corr = data.corr(numeric_only=True)\n",
    "\n",
    "# Create an upper-triangular mask to avoid counting duplicates and diagonals\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "\n",
    "# Extract the absolute correlation values from the upper triangle\n",
    "corr_values = corr.where(mask).stack().abs()\n",
    "\n",
    "# Define the threshold to consider a correlation as \"strong\"\n",
    "threshold = 0.50\n",
    "\n",
    "# Calculate the percentage of variable pairs that exceed the threshold\n",
    "significant_share = (corr_values >= threshold).mean() * 100\n",
    "\n",
    "# Display the result with two decimal precision\n",
    "print(f\"Percentage of variable pairs with |r| ≥ {threshold}: {significant_share:.2f}%\")\n",
    "\n",
    "# Identify and display the top 10 strongest correlations\n",
    "top_corrs = corr_values.sort_values(ascending=False).head(10).round(3)\n",
    "print(\"\\nTop 10 strongest correlations:\\n\")\n",
    "print(top_corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary: Correlation Structure**\n",
    "\n",
    "The correlation analysis reveals strong internal relationships among variables,\n",
    "particularly within the *Technical Excellence & Innovation* dimension.  \n",
    "Approximately **11.6%** of variable pairs show |r| ≥ 0.5, confirming high internal\n",
    "consistency and suitability for factor extraction in the next stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Data Characteristics and Patterns\n",
    "\n",
    "The correlation analysis shows that the strongest relationships occur mainly among the **Technical Excellence & Innovation** dimensions.  \n",
    "Variables such as `system_integration`, `innovation_solutions`, `technical_documentation`, and `technical_expertise` exhibit **very high positive correlations (r ≈ 0.64–0.67)**, suggesting that they represent a shared underlying construct of technical capability and innovation strength.  \n",
    "\n",
    "Approximately **11.6% of all variable pairs** display correlations of |r| ≥ 0.50, indicating a **high degree of internal consistency** across satisfaction items.  \n",
    "This means that customers who rate the company highly in technical expertise and problem-solving also tend to perceive excellence in innovation and integration efforts.  \n",
    "\n",
    "Overall, the dataset reveals a **cohesive satisfaction structure**, dominated by strongly interconnected technical and innovation factors that jointly drive customers’ positive perceptions of service quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Factor Analysis Suitability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Framework for Factor Analysis Suitability**\n",
    "\n",
    "To assess whether the dataset is appropriate for factor analysis, three complementary statistical criteria are evaluated:\n",
    "\n",
    "1. **Sampling Adequacy**  \n",
    "   Measured through the **Observation-to-Variable Ratio (n/p)** and the **Kaiser-Meyer-Olkin (KMO) test**.  \n",
    "   High ratios (>10:1) and KMO values above 0.6 indicate sufficient shared variance and sample stability.\n",
    "\n",
    "2. **Sphericity of the Correlation Matrix**  \n",
    "   Assessed using **Bartlett’s Test of Sphericity**, which tests whether the correlation matrix significantly differs from an identity matrix.  \n",
    "   A significant p-value (< 0.05) confirms that correlations exist among variables, supporting factorability.\n",
    "\n",
    "3. **Intercorrelation Strength**  \n",
    "   Evaluated through the **percentage of correlations ≥ 0.3** and the **mean absolute correlation**.  \n",
    "   These measures confirm that items are sufficiently correlated to justify extracting underlying factors.\n",
    "\n",
    "Together, these three criteria provide a structured statistical framework for determining dataset suitability for factor analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Tests:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizes the dataset by removing missing values and scaling all variables to have mean 0 and standard deviation 1, ensuring comparability and stability in the factor analysis process.\n",
    "\n",
    "data = data.dropna()\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = data.shape[0]   # number of observations\n",
    "n_vars = data.shape[1]  # number of variables\n",
    "ratio = n_obs / n_vars\n",
    "\n",
    "print(f\"Observations: {n_obs}\")\n",
    "print(f\"Variables: {n_vars}\")\n",
    "print(f\"Observation-to-variable ratio: {ratio:.1f}:1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling Adequacy Ratio**\n",
    "\n",
    "The dataset includes **3,235 observations and 23 variables**, resulting in an **observation-to-variable ratio of approximately 140.7:1**.  \n",
    "This far exceeds the recommended minimum of **5:1** (Hair et al., 2019), confirming an **excellent sample size** for stable and reliable factor extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executes the Kaiser-Meyer-Olkin (KMO) test to assess the dataset’s adequacy for factor analysis, classifying the result based on threshold values to interpret the sampling suitability level.\n",
    "# Values greater than 0.6 indicate that the factor analysis is suitable.\n",
    "\n",
    "kmo_all, kmo_model = calculate_kmo(data)\n",
    "print(\"\\nKMO TEST: \")\n",
    "print(f\"KMO general: {kmo_model:.3f}\")\n",
    "print(\"Interpretación:\")\n",
    "if kmo_model >= 0.9:\n",
    "    print(\"Excelente adecuación para análisis factorial.\")\n",
    "elif kmo_model >= 0.8:\n",
    "    print(\"Muy buena adecuación para análisis factorial.\")\n",
    "elif kmo_model >= 0.7:\n",
    "    print(\"Adecuación aceptable.\")\n",
    "elif kmo_model >= 0.6:\n",
    "    print(\"Adecuación marginalmente aceptable.\")\n",
    "else:\n",
    "    print(\"Inadecuado para análisis factorial (KMO < 0.6).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate individual KMO values using the same function\n",
    "kmo_all, kmo_model = calculate_kmo(data)\n",
    "\n",
    "# Display the KMO for each variable\n",
    "kmo_per_variable = pd.Series(kmo_all, index=items)\n",
    "print(\"KMO values per variable:\\n\")\n",
    "print(kmo_per_variable.round(3))\n",
    "\n",
    "# Optional: visualize the lowest ones\n",
    "lowest_kmo = kmo_per_variable.sort_values().head(5)\n",
    "print(\"\\nVariables with the lowest individual KMO values:\\n\")\n",
    "print(lowest_kmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KMO per Variable — Summary**\n",
    "\n",
    "Individual KMO values were examined to verify the adequacy of each item for factor analysis.  \n",
    "\n",
    "All variables show KMO values well above the **0.60 threshold**, confirming that each contributes meaningfully to the shared variance structure. \n",
    " \n",
    "This supports the **overall KMO = 0.959**, reinforcing the dataset’s strong suitability for factor extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you overwrote `data` with a numpy array after scaling, rebuild a DataFrame.\n",
    "# Use the same item columns you used for the factor analysis subset.\n",
    "# Example assuming `items` holds the list of survey item names:\n",
    "try:\n",
    "    X_for_bartlett = pd.DataFrame(data, columns=items)\n",
    "except Exception:\n",
    "    # If the scaled array is unavailable, use the original unscaled DataFrame subset instead.\n",
    "    X_for_bartlett = df[items].dropna()\n",
    "\n",
    "# Compute Bartlett’s test of sphericity\n",
    "chi2, p_value = calculate_bartlett_sphericity(X_for_bartlett)\n",
    "\n",
    "# Display test results\n",
    "print(\"BARTLETT'S TEST OF SPHERICITY\")\n",
    "print(f\"Chi-square: {chi2:,.2f}\")\n",
    "print(f\"p-value: {p_value:.4e}\")\n",
    "\n",
    "\n",
    "# Interpret the result:\n",
    "# If the p-value is below 0.05, the null hypothesis of an identity matrix is rejected.\n",
    "# This means correlations exist among variables, making factor analysis appropriate.\n",
    "if p_value < 0.05:\n",
    "    print(\"Result: Significant (p < 0.05) — the correlation matrix is not an identity matrix; \"\n",
    "          \"factor analysis is appropriate.\")\n",
    "else:\n",
    "    print(\"Result: Not significant — correlations resemble an identity matrix; \"\n",
    "          \"factor analysis may not be appropriate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bartlett’s Test of Sphericity — Summary**\n",
    "\n",
    "Bartlett’s test is **significant (p < 0.05)**, indicating that the correlation matrix is **not** an identity matrix.  \n",
    "Therefore, the variables share common variance and the dataset is **appropriate for factor analysis** (in line with the KMO result).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the percentage of strong correlations (r ≥ 0.3) and the mean of absolute correlations, helping assess whether variables are sufficiently interrelated to justify a factor analysis.\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))  \n",
    "corr_no_diag = corr.where(~mask)\n",
    "\n",
    "porcentaje_corr_fuertes = (abs(corr_no_diag) > 0.3).sum().sum() / ((len(corr)**2 - len(corr)) / 2) * 100\n",
    "\n",
    "print(\"\\nCORRELATION ASSESSMENT: \")\n",
    "print(f\"Correlaciones con |r| ≥ 0.3: {porcentaje_corr_fuertes:.1f}% del total\")\n",
    "print(\"Promedio de correlaciones absolutas:\", corr.mean().round(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes descriptive statistics of the correlation matrix, including the mean, maximum, and minimum correlation values, to verify the overall strength and variability of relationships among variables before performing factor analysis.\n",
    "\n",
    "corr_mean = corr.where(~np.eye(corr.shape[0], dtype=bool)).mean().mean()\n",
    "corr_max = corr.where(~np.eye(corr.shape[0], dtype=bool)).max().max()\n",
    "corr_min = corr.where(~np.eye(corr.shape[0], dtype=bool)).min().min()\n",
    "\n",
    "print(\"\\nBASIC ASSUMPTIONS: \")\n",
    "print(f\"Media de correlaciones absolutas: {corr_mean:.3f}\")\n",
    "print(f\"Máxima correlación: {corr_max:.3f}\")\n",
    "print(f\"Mínima correlación: {corr_min:.3f}\")\n",
    "print(\"Distribución de correlaciones:\")\n",
    "corr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Factor Analysis Suitability — Interpretation**\n",
    "\n",
    "\n",
    "**Is the dataset suitable for factor analysis?**  \n",
    "Yes. The dataset shows excellent suitability for factor analysis based on the three evaluation criteria:  \n",
    "\n",
    "1. **Sampling Adequacy:**  \n",
    "   - The **observation-to-variable ratio** is **3,235 / 23 = 140.7:1**, which far exceeds the recommended 10:1 minimum, confirming an excellent sample size.  \n",
    "   - The **overall KMO = 0.959**, indicating outstanding sampling adequacy.  \n",
    "   - All **individual KMO values** exceed 0.60, showing that every variable contributes meaningfully to the shared variance structure.  \n",
    "\n",
    "2. **Sphericity of the Correlation Matrix:**  \n",
    "   - **Bartlett’s Test of Sphericity** is **significant (p < 0.05)**, meaning the correlation matrix is not an identity matrix and factor analysis is statistically appropriate.  \n",
    "\n",
    "3. **Intercorrelation Strength:**  \n",
    "   - Nearly **48.2% of all correlations** have |r| ≥ 0.3, confirming that many variables share common variance.  \n",
    "   - The **average absolute correlation (0.34)** surpasses the 0.30 threshold typically required for a valid factor structure.  \n",
    "\n",
    "Together, these results confirm that the dataset meets all statistical assumptions for applying factor analysis with high confidence and robustness.\n",
    "\n",
    "\n",
    "\n",
    "**What do the initial patterns suggest about underlying factors?**  \n",
    "\n",
    "The correlation structure reveals several clusters of related variables, suggesting the presence of multiple underlying latent dimensions:  \n",
    "\n",
    "- The strongest correlations (**r ≈ 0.65–0.77**) appear among variables such as  \n",
    "  `technical_expertise`, `problem_solving`, `innovation_solutions`, `technical_documentation`, and `system_integration`,  \n",
    "  forming a **Technical Excellence & Innovation** factor.  \n",
    "- Moderate correlations among `project_management`, `budget_control`, and `quality_deliverables` point to a **Project Delivery & Quality Assurance** dimension.  \n",
    "- Lower but consistent correlations among relationship-oriented items (`account_manager_responsive`, `trust_reliability`, `training_quality`) suggest a **Relationship Management & Service Excellence** factor.  \n",
    "\n",
    "Overall, the correlation patterns indicate that customer satisfaction toward **TechnoServe Solutions** is primarily driven by three coherent latent constructs: **technical quality, project performance, and client relationship excellence**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2: Factor Extraction and Determination**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Determining Number of Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the Factor Analysis model and computes eigenvalues to determine how much variance each factor explains, helping identify the optimal number of factors to retain for analysis.\n",
    "\n",
    "# Calculate eigenvalues\n",
    "fa = FactorAnalyzer(rotation=None)\n",
    "fa.fit(data)\n",
    "\n",
    "eigenvalues, vectors = fa.get_eigenvalues()\n",
    "\n",
    "print(\"Eigenvalues:\\n\", eigenvalues.round(3))\n",
    "print(f\"\\n Number of eigenvalues with eigenvalue > 1: {(eigenvalues > 1).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a Scree Plot to visualize eigenvalues and identify the optimal number of factors, using the Kaiser Criterion (eigenvalue ≥ 1) as a reference for factor retention.\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o')\n",
    "plt.title(\"Scree Plot — Determination of the number of factors\", fontsize=13)\n",
    "plt.xlabel(\"Factor\")\n",
    "plt.ylabel(\"Eigenvalue\")\n",
    "plt.axhline(y=1, color='red', linestyle='--', label=\"Kaiser Criterion (Eigenvalue=1)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"scree_plot.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Choose a number of factors \n",
    "fa_fixed = FactorAnalyzer(n_factors=5, method='principal', rotation=None)\n",
    "fa_fixed.fit(data)\n",
    "\n",
    "# Get variance explained by each factor \n",
    "variance, prop_var, cum_var = fa_fixed.get_factor_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and displays the variance explained by each factor and the cumulative variance, helping assess how much of the dataset’s total variability is captured by the selected factors.\n",
    "\n",
    "variance_df = pd.DataFrame({\n",
    "    \"Factor\": [f\"Factor{i+1}\" for i in range(fa_fixed.n_factors)],\n",
    "    \"Proportion Variance\": prop_var,\n",
    "    \"Cumulative Variance\": cum_var\n",
    "})\n",
    "\n",
    "print(\" Explained variance and cumulative variance for the 5 factors:\\n\")\n",
    "print(variance_df.round(3))\n",
    "\n",
    "# Show final cumulative total\n",
    "print(f\"\\n Total explained variance for the 5 factors: {cum_var[-1]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine the optimal number of factors and justify your choice**\n",
    "\n",
    "The eigenvalues indicate that five factors have values greater than 1.0, suggesting that each explains a meaningful portion of the total variance in the dataset.\n",
    "\n",
    "The Scree Plot supports that a clear inflection point appears after the fifth factor, where the curve begins to flatten. This pattern implies that additional factors would contribute minimal explanatory power, reinforcing the decision to retain five factors.\n",
    "\n",
    "The explained variance table shows that Factor 1 accounts for 38.0% of the variance, while Factors 2, 3, 4, and 5 contribute 7.7%, 6.2%, 5.2%, and 4.7%, respectively.\n",
    "Together, the five factors explain 61.85% of the total variance, surpassing the commonly accepted 60% threshold for exploratory factor analysis in social and behavioral sciences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Factor Extraction and Rotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs factor extraction using the Principal Factor method with five components, then generates a factor loading matrix that shows how each variable contributes to each extracted factor.\n",
    " \n",
    "fa = FactorAnalyzer(n_factors=5, method='principal', rotation=None)\n",
    "fa.fit(data)\n",
    "\n",
    "data_df=pd.DataFrame(data, columns=items)\n",
    "pd.DataFrame(fa.loadings_, index=data_df.columns, columns=['Factor1','Factor2','Factor3','Factor4','Factor5'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies Varimax rotation to simplify the factor structure, enhancing interpretability by maximizing high loadings and minimizing low ones for each variable across factors.\n",
    "\n",
    "rotator_varimax = Rotator(method='varimax')\n",
    "Lambda_varimax = rotator_varimax.fit_transform(fa.loadings_)\n",
    "\n",
    "print(\"Factor Loadings after Varimax Rotation:\")\n",
    "loads_varimax = pd.DataFrame(Lambda_varimax, index=data_df.columns, \n",
    "                             columns=['Factor1','Factor2','Factor3','Factor4','Factor5'])\n",
    "display(loads_varimax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applies promax rotation to allow for correlated factors, enhancing interpretability by permitting factors to correlate, which often reflects real-world data structures more accurately.\n",
    "rotator_promax = Rotator(method='promax')\n",
    "Lambda_promax = rotator_promax.fit_transform(fa.loadings_)\n",
    "\n",
    "print(\"\\nFactor Loadings after Promax Rotation:\")\n",
    "loads_promax = pd.DataFrame(Lambda_promax, index=data_df.columns, \n",
    "                            columns=['Factor1','Factor2','Factor3','Factor4','Factor5'])\n",
    "display(loads_promax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compares correlartion between factors after promax rotation to understand inter-factor relationships and assess the degree of correlation among the extracted factors.\n",
    "corr_matrix = pd.DataFrame(np.corrcoef(Lambda_promax.T),\n",
    "                           index=['F1','F2','F3','F4','F5'],\n",
    "                           columns=['F1','F2','F3','F4','F5'])\n",
    "\n",
    "print(\"\\nFactor Correlation Matrix (Promax Rotation):\")\n",
    "display(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies Varimax rotation to simplify the factor structure, enhancing interpretability by maximizing high loadings and minimizing low ones for each variable across factors.\n",
    "\n",
    "rotator = Rotator()\n",
    "Lambda_rot = rotator.fit_transform(fa.loadings_)\n",
    "\n",
    "print(\"Cargas factoriales tras rotación Varimax:\")\n",
    "loads_rotados = pd.DataFrame(Lambda_rot, index=data_df.columns, columns=['Factor1','Factor2','Factor3','Factor4','Factor5'])\n",
    "loads_rotados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the communality (shared variance explained by the factors) and uniqueness (specific variance not explained) for each variable, summarizing how well each item is represented in the factor model.\n",
    "\n",
    "communalities = fa.get_communalities()\n",
    "uniqueness = fa.get_uniquenesses()\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Comunalidad (h^2)': communalities,\n",
    "    'Unicidad (ψ)': uniqueness\n",
    "}, index=data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates factor scores for each observation, representing how strongly each case loads onto the extracted factors, and stores them in a DataFrame for further interpretation or modeling.\n",
    "\n",
    "fa_scores = fa.transform(data)\n",
    "df_scores=pd.DataFrame(fa_scores, columns=['Factor1','Factor2','Factor3','Factor4','Factor5'])\n",
    "df_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Factor Extraction and Determination - Interpretation**\n",
    "\n",
    "**How many factors best represent the data?**\n",
    "\n",
    "According to the analysis results, five factors adequately explain the structure of the dataset. The Scree Plot shows a clear inflection point (“elbow”) after the fifth component, indicating that additional factors contribute minimal variance. Moreover, the first five factors have eigenvalues greater than 1.0 according to the Kaiser criterion, and together they explain approximately 61.85% of the total variance, which is an acceptable level for perception or survey-based data.\n",
    "Therefore, retaining five factors is both statistically and theoretically appropriate to describe the main dimensions of customer satisfaction.\n",
    "\n",
    "*Comparison of Rotation Methods*\n",
    "Both Varimax (orthogonal) and Promax (oblique) rotations were tested to identify the most interpretable and conceptually sound structure.\n",
    "The Varimax rotation produced a clear and distinct factor structure, where each variable loads strongly on a single factor and cross-loadings are minimized.\n",
    "The Promax rotation revealed a similar pattern but introduced moderate inter-factor correlations (ranging from approximately –0.29 to +0.27), suggesting that while factors share some conceptual relationships, they remain largely independent.\n",
    "\n",
    "Given these findings, Varimax was selected as the final rotation method because it provides a simpler, more interpretable solution while maintaining factor independence—making it more suitable for business interpretation and communication.\n",
    "\n",
    "**What does each factor represent in business terms?**\n",
    "\n",
    "\n",
    "- **Factor 1:** Groups variables such as technical_expertise, problem_solving, innovation_solutions, technical_documentation, and system_integration, reflecting the Technical Excellence & Innovation dimension.\n",
    "\n",
    "- **Factor 2:** Includes trust_reliability, long_term_partnership, and communication_clarity, associated with Relationship Management & Customer Trust.\n",
    "\n",
    "- **Factor 3:** Combines value_for_money, cost_transparency, roi_demonstration, and competitive_pricing, representing Perceived Value & Financial Transparency.\n",
    "\n",
    "- **Factor 4:** Concentrates project_management, timeline_adherence, budget_control, and quality_deliverables, related to Project Execution & Delivery Performance.\n",
    "\n",
    "- **Factor 5:** Groups support_responsiveness, training_quality, and documentation_help, reflecting Customer Support & Service Excellence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The rotated five-factor model demonstrates a stable, interpretable, and business-relevant structure.\n",
    "While the Promax rotation confirmed slight correlations among factors, the Varimax rotation was ultimately preferred for its clarity and independence—providing a robust foundation for interpreting the key drivers of customer satisfaction and designing actionable business strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Interpretation and Business Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Factor Interpretation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factor representation for TechnoServe Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies variables with significant loadings (|≥0.4|) for each factor and labels them according to their underlying meaning, providing interpretable names such as Technical Innovation, Economic Transparency, and Client Trust.\n",
    "\n",
    "# --- FACTOR 1 ---\n",
    "loads_f1 = loads_rotados['Factor1']\n",
    "print(\"Variables with high loadings (> |0.4|) F1:\")\n",
    "print(loads_f1[loads_f1.abs() >= 0.4])\n",
    "loads_f1 = loads_f1[loads_f1.abs() >= 0.4]\n",
    "loads_f1.name = 'Competencia en Innovación y Solución Técnica'\n",
    "\n",
    "# --- FACTOR 2 ---\n",
    "loads_f2 = loads_rotados['Factor2']\n",
    "print(\"\\nVariables with high loadings (> |0.4|) F2:\")\n",
    "print(loads_f2[loads_f2.abs() >= 0.4])\n",
    "loads_f2 = loads_f2[loads_f2.abs() >= 0.4]\n",
    "loads_f2.name = 'Transparencia y Valor Económico'\n",
    "\n",
    "# --- FACTOR 3 ---\n",
    "loads_f3 = loads_rotados['Factor3']\n",
    "print(\"\\nVariables with high loadings (> |0.4|) F3:\")\n",
    "print(loads_f3[loads_f3.abs() >= 0.4])\n",
    "loads_f3 = loads_f3[loads_f3.abs() >= 0.4]\n",
    "loads_f3.name = 'Relación y Confianza con el Cliente'\n",
    "\n",
    "# --- FACTOR 4 ---\n",
    "loads_f4 = loads_rotados['Factor4']\n",
    "print(\"\\nVariables with high loadings (> |0.4|) F4:\")\n",
    "print(loads_f4[loads_f4.abs() >= 0.4])\n",
    "loads_f4 = loads_f4[loads_f4.abs() >= 0.4]\n",
    "loads_f4.name = 'Ejecución de Proyecto y Entrega'  \n",
    "\n",
    "# --- FACTOR 5 ---\n",
    "loads_f5 = loads_rotados['Factor5']\n",
    "print(\"\\nVariables with high loadings (> |0.4|) F5:\")\n",
    "print(loads_f5[loads_f5.abs() >= 0.4])\n",
    "loads_f5 = loads_f5[loads_f5.abs() >= 0.4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask where each value is True if |loading| ≥ 0.40\n",
    "high_loadings_mask = loads_rotados.abs() >= 0.4\n",
    "\n",
    "# Count how many factors each variable loads on significantly\n",
    "cross_load_counts = high_loadings_mask.sum(axis=1)\n",
    "\n",
    "# Identify variables with cross-loadings (appearing in ≥ 2 factors)\n",
    "cross_loaded_vars = cross_load_counts[cross_load_counts >= 2]\n",
    "\n",
    "print(\"\\nVariables with significant cross-loadings (|loading| ≥ 0.4 in ≥ 2 factors):\")\n",
    "print(cross_loaded_vars)\n",
    "\n",
    "# Optional: Display their loadings for detailed inspection\n",
    "if not cross_loaded_vars.empty:\n",
    "    print(\"\\nDetailed loadings for cross-loaded variables:\")\n",
    "    print(loads_rotados.loc[cross_loaded_vars.index])\n",
    "else:\n",
    "    print(\"\\nNo variables show significant cross-loadings (|loading| ≥ 0.4 in ≥ 2 factors).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Loading Analysis — Interpretation**\n",
    "\n",
    "Cross-loading analysis identifies variables that load strongly (|≥ 0.4|) on more than one factor, indicating potential overlap between constructs. \n",
    "\n",
    "In this dataset, only a few items (e.g., `project_management`, `quality_deliverables`) show moderate cross-loadings between **Technical Excellence & Innovation** and **Project Execution & Delivery**, suggesting a conceptual link between technical capability and project performance.  \n",
    "\n",
    "However, the overall structure remains interpretable since most variables load cleanly on a single dominant factor.  \n",
    "\n",
    "Cross-loadings will be monitored in subsequent validation stages to ensure factor discriminability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for significant loading\n",
    "threshold = 0.4\n",
    "\n",
    "# Count how many factors each variable loads on significantly\n",
    "load_counts = (loads_rotados.abs() >= threshold).sum(axis=1)\n",
    "\n",
    "# Variables with simple structure: load on exactly one factor (no cross-loadings)\n",
    "simple_vars = load_counts[load_counts == 1]\n",
    "complex_vars = load_counts[load_counts > 1]\n",
    "\n",
    "# Calculate percentages\n",
    "total_vars = len(load_counts)\n",
    "pct_simple = len(simple_vars) / total_vars * 100\n",
    "pct_complex = len(complex_vars) / total_vars * 100\n",
    "\n",
    "print(f\"Total variables: {total_vars}\")\n",
    "print(f\"Variables with simple structure (loading ≥ {threshold} on 1 factor): {len(simple_vars)} ({pct_simple:.1f}%)\")\n",
    "print(f\"Variables with complex structure (cross-loadings ≥ {threshold}): {len(complex_vars)} ({pct_complex:.1f}%)\")\n",
    "\n",
    "# Optionally, list complex variables for review\n",
    "if not complex_vars.empty:\n",
    "    print(\"\\nVariables with complex loadings:\")\n",
    "    print(complex_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Factor Solution Quality — Simple Structure Evaluation**\n",
    "\n",
    "To assess the clarity of the factor solution, the proportion of variables exhibiting a *simple structure* was computed.  \n",
    "\n",
    "Variables were classified as “simple” when they loaded significantly (|≥ 0.4|) on only one factor and showed no relevant cross-loadings.\n",
    "\n",
    "\n",
    "Results show that **approximately 87% of the variables** present a simple structure, while only **13%** exhibit complex or ambiguous loadings.  \n",
    "\n",
    "This confirms that the extracted factor solution is **statistically clean, well-differentiated, and highly interpretable**, satisfying the criteria of factor simplicity and discriminant validity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Business Insights and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Factor scores** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a DataFrame to store factor scores for each customer, aligning them with the original dataset index and naming columns according to the rotated factor structure for further analysis and interpretation.\n",
    "\n",
    "customers_factors = pd.DataFrame(\n",
    "    fa_scores,\n",
    "    index=data_obj.index, # Use the index from data_obj\n",
    "    columns=loads_rotados.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict outcome variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines the factor scores with key business outcome variables (e.g., satisfaction, NPS, renewal rate) to create a unified dataset that enables correlation and regression analyses between latent factors and customer performance metrics.\n",
    "\n",
    "outcomes = [\n",
    "    \"overall_satisfaction\",\n",
    "    \"nps_score\",\n",
    "    \"renewal_likelihood\",\n",
    "    \"revenue_growth_pct\",\n",
    "    \"referrals_generated\"\n",
    "]\n",
    "\n",
    "data_new = pd.read_csv(\"customer_satisfaction_data.csv\")\n",
    "data_new = data_new.dropna()\n",
    "df_model = pd.concat([data_new[outcomes].reset_index(drop=True), customers_factors.reset_index(drop=True)], axis=1)\n",
    "\n",
    "df_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extended Scope of Predictive Modeling**\n",
    "\n",
    "In the initial analysis, only `overall_satisfaction` was modeled as the dependent variable.  \n",
    "To address this limitation, the predictive analysis has now been extended to include all five business outcomes:\n",
    "`nps_score`, `renewal_likelihood`, `revenue_growth_pct`, and `referrals_generated`.  \n",
    "\n",
    "This enhancement provides a broader understanding of how each latent factor impacts both\n",
    "**customer perception** (satisfaction, NPS) and **business performance** (renewal, revenue, referrals).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all business outcome variables\n",
    "outcomes = [\n",
    "    \"overall_satisfaction\",\n",
    "    \"nps_score\",\n",
    "    \"renewal_likelihood\",\n",
    "    \"revenue_growth_pct\",\n",
    "    \"referrals_generated\",\n",
    "]\n",
    "\n",
    "# Create a dictionary to store regression results\n",
    "results = []\n",
    "\n",
    "# Loop through each outcome variable and fit a separate linear regression model\n",
    "for target in outcomes:\n",
    "    y = df_model[target]\n",
    "    X = df_scores  # use the same factor scores as predictors\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    \n",
    "    # Store results\n",
    "    for factor_name, coef in zip(X.columns, model.coef_):\n",
    "        results.append({\n",
    "            \"Outcome\": target,\n",
    "            \"Factor\": factor_name,\n",
    "            \"Coefficient\": coef,\n",
    "            \"R²\": r2,\n",
    "            \"RMSE\": rmse\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "predictive_results = pd.DataFrame(results)\n",
    "\n",
    "# Display summary of model performance\n",
    "summary_perf = predictive_results.groupby(\"Outcome\")[[\"R²\", \"RMSE\"]].mean().round(3)\n",
    "print(\"\\n Predictive Model Performance per Outcome \")\n",
    "print(summary_perf)\n",
    "\n",
    "# Show ranked importance of factors for each outcome\n",
    "print(\"\\nFactor Impact by Outcome \")\n",
    "display(predictive_results.sort_values(by=[\"Outcome\", \"Coefficient\"], ascending=[True, False]).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesizes the overall importance of each latent factor across all outcome variables\n",
    "\n",
    "cross_outcome_importance = (\n",
    "    predictive_results\n",
    "    .groupby(\"Factor\")[\"Coefficient\"]\n",
    "    .apply(lambda x: x.abs().mean())  # mean absolute effect size\n",
    "    .sort_values(ascending=False)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Coefficient\": \"Mean_Absolute_Impact\"})\n",
    ")\n",
    "\n",
    "print(\"\\n Cross-Outcome Factor Prioritization \")\n",
    "print(cross_outcome_importance.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Outcome Prioritization — Interpretation**\n",
    "\n",
    "This synthesis ranks factors based on their **average absolute influence across all business outcomes**, providing a single prioritized view of impact:\n",
    "\n",
    "1. **Technical Excellence & Innovation (Factor 1)** — Strongest and most consistent driver across all outcomes, explaining satisfaction, NPS, and renewal likelihood simultaneously.  \n",
    "2. **Financial Transparency & Value (Factor 2)** — Secondary importance, mainly influencing renewal and revenue growth.  \n",
    "3. **Project Execution & Delivery (Factor 4)** — Moderate influence, supporting customer trust through timely and high-quality delivery.  \n",
    "4. **Relationship Management & Trust (Factor 3)** — Lower global impact, but crucial for referrals and NPS.  \n",
    "5. **Customer Support & Service (Factor 5)** — Limited statistical weight, suggesting improvement opportunities.\n",
    "\n",
    "This cross-outcome view allows TechnoServe Solutions to **prioritize investment and training** in the most impactful areas (technical excellence and transparency) while strategically strengthening client relationships and service support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extended Predictive Analysis — Interpretation**\n",
    "\n",
    "The extended regression models evaluate how the extracted factors predict **five business outcomes** instead of only `overall_satisfaction`.  \n",
    "\n",
    "**Model Performance Summary:**  \n",
    "- R² values range between **0.42 and 0.61**, indicating moderate to strong predictive capability across outcomes.  \n",
    "- RMSE values remain below 0.55, confirming stable model accuracy.  \n",
    "\n",
    "**Key Insights:**  \n",
    "- **Technical Excellence & Innovation (Factor 1)** consistently shows the strongest positive effect across all outcomes, reinforcing its central role in client satisfaction, renewal, and referral potential.  \n",
    "- **Value & Financial Transparency (Factor 2)** contributes notably to `renewal_likelihood` and `revenue_growth_pct`, showing that transparent pricing builds client retention.  \n",
    "- **Relationship Management & Trust (Factor 3)** primarily drives `nps_score` and `referrals_generated`, highlighting the impact of interpersonal connection and client confidence.  \n",
    "\n",
    "Overall, expanding the predictive analysis to all five outcomes provides a **comprehensive view** of how each latent dimension influences both **emotional (satisfaction, trust)** and **financial (renewal, growth)** metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictive Modeling Extension**\n",
    "\n",
    "Originally, the study only included correlation analysis across outcomes without predictive modeling.  \n",
    "\n",
    "This updated version introduces **regression-based prediction for all five outcomes**,  enabling a more robust understanding of how each factor drives customer satisfaction, NPS, renewal likelihood, referrals, and revenue growth.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the correlation matrix between extracted factors and business outcome variables to identify which latent dimensions have the strongest relationships with customer satisfaction, retention, and revenue growth.\n",
    "\n",
    "corr = df_model.corr().loc[\n",
    "    df_scores.columns,  # Line: factores\n",
    "    outcomes             # Column: outcomes\n",
    "]\n",
    "\n",
    "print(\"\\nCorrelations (Factors vs Outcomes):\")\n",
    "corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies a linear regression model to evaluate how well the extracted factors predict overall customer satisfaction. \n",
    "# It reports the model's performance using R² and RMSE metrics, and displays the coefficients to identify which factors have the greatest influence on satisfaction.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "X = df_scores  # tus factores\n",
    "y = df_model['overall_satisfaction']  # variable de salida\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "r2 = r2_score(y, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "print(f\"R²: {r2:.3f}, RMSE: {rmse:.3f}\")\n",
    "coefficients = pd.DataFrame({\n",
    "    \"Factor\": X.columns,\n",
    "    \"Coefficient\": model.coef_\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Renewal Likelihood**\n",
    "\n",
    "Although *renewal_likelihood* was included in the correlation stage, no regression model or interpretation was developed for this variable.  \n",
    "Since it represents a critical indicator of **client retention and long-term loyalty**, a future extension should incorporate a predictive model focused on this outcome  to better understand which factors influence renewal probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify which factors are most important for business outcomes**\n",
    "\n",
    "The linear regression model reveals that Factor 1 (Technical Excellence & Innovation) is by far the strongest driver of overall customer satisfaction, with a coefficient of 0.64, explaining most of the predictive power of the model (R² = 0.60).  \n",
    "This indicates that clients primarily value the company’s ability to deliver high-quality technical solutions, solve complex problems, and innovate efficiently.  \n",
    "\n",
    "Other factors such as Project Delivery & Quality Assurance (Factor 4) and Value & Financial Transparency (Factor 2) also contribute positively, although with smaller impacts, suggesting that operational reliability and financial clarity reinforce satisfaction once technical trust is established.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategic Recomendations**\n",
    "\n",
    "**Prioritize factors based on business impact**\n",
    "- Highest priority: Strengthen Technical Excellence & Innovation through continuous improvement of engineering standards, faster problem-solving processes, and innovative solution design.  \n",
    "\n",
    "- Secondary priorities: Maintain high standards in Project Delivery and Financial Transparency, as they reinforce customer trust and retention.\n",
    "\n",
    "**Specific improvement strategies**\n",
    "- Technical Excellence & Innovation: \n",
    "\n",
    "  TechnoServe Solutions should invest in R&D and internal technical training programs to strengthen its innovation capacity and maintain high technical standards. Additionally, it should implement cross-functional innovation teams that collaborate across departments to design faster, data-driven solutions aligned with client needs. Finally, the company should promote documentation and knowledge-sharing practices to ensure that technical expertise is scalable, consistent, and accessible throughout the organization.\n",
    "\n",
    "\n",
    "- Project Delivery & Quality Assurance: \n",
    "\n",
    "  TechnoServe Solutions should standardize its project management frameworks, adopting methodologies such as Agile or PMI-based approaches to improve predictability and consistency in project execution. Additionally, the company should track timeline adherence and deliverable quality through real-time dashboards, enabling proactive monitoring, quicker decision-making, and continuous improvement in service delivery. \n",
    "\n",
    "- Value & Financial Transparency:  \n",
    "  \n",
    "  TechnoServe Solutions should enhance billing accuracy and ensure clear ROI communication in all client reports to strengthen financial trust and transparency. Furthermore, the company can offer cost-benefit visualizations that clearly illustrate how pricing aligns with delivered value, helping clients better understand the economic impact of TechnoServe’s solutions and reinforcing confidence in the company’s financial practices.  \n",
    "\n",
    "\n",
    "\n",
    "**Action plan for TechnoServe Solutions**\n",
    "\n",
    "| Timeframe | Strategic Focus | Key Actions |\n",
    "|------------|----------------|-------------|\n",
    "| **Short-term (0–6 months)** | Improve client perception of technical reliability | Launch rapid technical support task force; update documentation quality standards. |\n",
    "| **Mid-term (6–12 months)** | Strengthen project and financial transparency | Deploy new KPI dashboards for project performance and cost tracking. |\n",
    "| **Long-term (1–2 years)** | Foster innovation culture and scalability | Establish innovation labs and feedback loops between clients and R&D teams. |\n",
    "\n",
    "\n",
    "**Cross-Outcome Prioritization**\n",
    "\n",
    "While each recommendation aligns with factor-specific insights, the action plan does not yet prioritize initiatives based on their **combined impact across multiple business outcomes** (*overall satisfaction, NPS, renewal, referrals, and revenue growth*).\n",
    "\n",
    "A cross-outcome prioritization matrix should be developed to identify which strategic dimensions (e.g., Technical Excellence, Financial Transparency, Relationship Management) yield the strongest **aggregate benefit** across all key metrics.  \n",
    "\n",
    "This approach would allow TechnoServe Solutions to allocate resources more efficiently, focusing first on actions that drive improvement in several outcomes simultaneously  rather than optimizing one metric at a time.\n",
    "\n",
    "\n",
    "**ROI Quantification**\n",
    "\n",
    "The current recommendations outline strategic actions but **lack quantified ROI estimates** for each proposed initiative. Including a projected **return on investment (ROI)**  would provide decision-makers with a clearer understanding of the **expected financial impact**  and help justify resource allocation across projects.\n",
    "\n",
    "Future iterations should integrate basic ROI modeling, estimating potential gains in **client retention, revenue growth, or operational efficiency** attributable to each recommendation.  This quantification would enhance the business relevance of the analysis  and strengthen the prioritization of initiatives based on measurable value creation.\n",
    "\n",
    "\n",
    "**Monitoring Metrics – Pending Definition**\n",
    "\n",
    "The current action plan lists strategic initiatives but **does not specify measurable KPIs or follow-up metrics**  to track progress over time. Defining clear indicators for each recommendation would enable continuous monitoring  and data-driven evaluation of implementation success.\n",
    "\n",
    "Future iterations should associate **specific metrics** with each initiative, such as:  \n",
    "- % improvement in client satisfaction or NPS after implementing technical training programs  \n",
    "- Increase in renewal rate following transparency and pricing reviews  \n",
    "- Reduction in response time or support tickets after service workflow optimization  \n",
    "\n",
    "Including these measurable indicators would allow TechnoServe Solutions to establish a **feedback loop for continuous improvement**, ensuring accountability and tangible business outcomes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Interpretation and Business Application - Interpretation**\n",
    "\n",
    "- Which factors drive customer satisfaction most?\n",
    "\n",
    "Customer satisfaction at TechnoServe Solutions is primarily driven by Factor 1, which shows the strongest positive effect (β = 0.64) on overall satisfaction.  \n",
    "Clients value the company’s ability to deliver high-quality, innovative, and well-documented technical solutions that effectively solve problems.  \n",
    "\n",
    "Secondary drivers include Factor 4 and Factor 2, which reinforce satisfaction through reliable execution and trust in fair pricing and ROI communication.\n",
    "\n",
    "\n",
    "- What specific actions should TechnoServe take?\n",
    "\n",
    "1. Enhance Technical Excellence & Innovation\n",
    "   - Expand internal technical training and knowledge-sharing programs.  \n",
    "   - Encourage innovation teams to design faster and more customized solutions.  \n",
    "   - Strengthen technical documentation and solution quality control.  \n",
    "\n",
    "2. Improve Project Delivery & Quality Assurance\n",
    "   - Adopt Agile or PMI-based frameworks to ensure on-time, high-quality delivery.  \n",
    "   - Implement KPI dashboards for tracking project progress and deliverable standards.  \n",
    "\n",
    "3. Increase Financial Transparency\n",
    "   - Provide clear ROI reports and transparent cost breakdowns to clients.  \n",
    "   - Regularly audit billing accuracy and communicate cost efficiency clearly.  \n",
    "\n",
    "By focusing on these actions, TechnoServe can increase client trust, satisfaction, and long-term retention, strengthening its competitive advantage in the consulting market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Factor Loadings Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an interactive Dash app that visualizes the rotated factor loadings matrix as a heatmap. \n",
    "\n",
    "app = Dash(__name__)\n",
    "server = app.server\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H3(\"Interactive Factor Loadings Viewer\"),\n",
    "    html.Label(\"Min |loading| threshold:\"),\n",
    "    dcc.Slider(0, 1, 0.05, value=0.4, id=\"thr\"),\n",
    "    dcc.Graph(id=\"heatmap\")\n",
    "])\n",
    "\n",
    "@app.callback(Output(\"heatmap\", \"figure\"), Input(\"thr\", \"value\"))\n",
    "def update_heatmap(thr):\n",
    "    df = loads_rotados.copy()\n",
    "    df[np.abs(df) < thr] = np.nan  # mask below threshold\n",
    "    fig = px.imshow(\n",
    "        df,\n",
    "        color_continuous_scale=\"RdBu_r\",\n",
    "        zmin=-1, zmax=1,\n",
    "        labels=dict(x=\"Factors\", y=\"Variables\", color=\"Loading\"),\n",
    "        title=f\"Factor Loadings Heatmap (|loading| ≥ {thr:.2f})\"\n",
    "    )\n",
    "    fig.update_layout(template=\"plotly_white\", height=600, title_x=0.5)\n",
    "    return fig\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a scree plot to visualize eigenvalues for factor selection. \n",
    "# The plot helps identify the optimal number of factors by showing where the curve flattens, with the red line (Kaiser Criterion) marking factors with eigenvalues greater than 1.\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, 'o-', color='royalblue')\n",
    "plt.axhline(y=1, color='red', linestyle='--', label='Kaiser Criterion (eigenvalue=1)')\n",
    "plt.title(\"Scree Plot for Factor Selection\", fontsize=14)\n",
    "plt.xlabel(\"Factor Number\")\n",
    "plt.ylabel(\"Eigenvalue\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a scree plot to visualize eigenvalues for factor selection. \n",
    "# The plot helps identify the optimal number of factors by showing where the curve flattens, with the red line (Kaiser Criterion) marking factors with eigenvalues greater than 1.\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "df_long = df_scores.melt(var_name=\"Factor\", value_name=\"Score\")\n",
    "\n",
    "fig = px.box(\n",
    "    df_long,\n",
    "    x=\"Factor\", y=\"Score\", color=\"Factor\",\n",
    "    points=\"all\",\n",
    "    title=\"Distribution of Factor Scores Across Customers\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis_title=\"Factors\",\n",
    "    yaxis_title=\"Standardized Score\",\n",
    "    title_x=0.5,\n",
    "    width=900, height=600\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Impact Summary Chart**\n",
    "\n",
    "The following table summarizes how each latent factor influences customer satisfaction outcomes, combining statistical relevance with business interpretation.\n",
    "\n",
    "\n",
    "| **Factor** | **Business Dimension** | **R² Contribution / Coefficient** | **Business Impact** |\n",
    "|-------------|------------------------|-----------------------------------|----------------------|\n",
    "| **Factor 1** | Technical Excellence & Innovation | 0.637 | Main driver of satisfaction, reflecting customers’ positive perception of TechnoServe’s technical quality, innovation, and system integration.|\n",
    "| **Factor 2** | Relationship Management & Trust | 0.026 |Moderate influence that builds long-term client loyalty and enhances perceived reliability. |\n",
    "| **Factor 4** | Project Execution & Delivery | 0.034 | Important for operational efficiency and meeting client expectations on timelines and deliverables. |\n",
    "| **Factor 5** | Customer Support & Service | 0.019 | Secondary effect that contributes to post-delivery satisfaction and responsiveness.|\n",
    "| **Factor 3** | Financial Transparency & Value | 0.012 |Lowest weight but still valuable for perceived fairness and ROI justification.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize factor importance (R² or standardized coefficients)\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "impact_df = pd.DataFrame({\n",
    "    \"Factor\": [\"Factor 1\", \"Factor 2\", \"Factor 3\", \"Factor 4\", \"Factor 5\"],\n",
    "    \"Business Dimension\": [\n",
    "        \"Technical Excellence & Innovation\",\n",
    "        \"Relationship Management & Trust\",\n",
    "        \"Financial Transparency & Value\",\n",
    "        \"Project Execution & Delivery\",\n",
    "        \"Customer Support & Service\"\n",
    "    ],\n",
    "    \"R2 Contribution\": [0.637, 0.026, 0.012, 0.034, 0.019]\n",
    "})\n",
    "\n",
    "fig = px.bar(\n",
    "    impact_df,\n",
    "    x=\"Factor\",\n",
    "    y=\"R2 Contribution\",\n",
    "    color=\"Factor\",\n",
    "    text=\"R2 Contribution\",\n",
    "    title=\"Relative Importance of Factors in Explaining Customer Satisfaction\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.update_traces(texttemplate=\"%{text:.3f}\", textposition=\"outside\")\n",
    "fig.update_layout(showlegend=False, yaxis_title=\"R² / Coefficient Weight\", xaxis_title=\"\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['satisfaction_proxy'] = data_df[[\n",
    "    'trust_reliability',\n",
    "    'quality_deliverables',\n",
    "    'timeline_adherence',\n",
    "    'support_responsiveness',\n",
    "    'value_for_money'\n",
    "]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between factor scores and satisfaction outcome\n",
    "\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "\n",
    "outcome_col = 'satisfaction_proxy'  \n",
    "\n",
    "for factor in df_scores.columns:\n",
    "    x = df_scores[factor]\n",
    "    y = data_df[outcome_col]\n",
    "    \n",
    "    X = sm.add_constant(x)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    r2 = model.rsquared\n",
    "\n",
    "    fig = px.scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        trendline=\"ols\",\n",
    "        color_discrete_sequence=[\"royalblue\"],\n",
    "        title=f\"Relationship Between {factor} and Customer Satisfaction (R² = {r2:.3f})\"\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        height=500,\n",
    "        width=800,\n",
    "        title_x=0.5,\n",
    "        xaxis_title=factor,\n",
    "        yaxis_title=\"Customer Satisfaction\",\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=4, opacity=0.6))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The scatter plots show how each latent factor score relates to overall customer satisfaction.\n",
    "Factor 1 demonstrates the strongest positive relationship, confirming its dominant influence on satisfaction.\n",
    "Other factors, such as Project Delivery (Factor 4) and Customer Support (Factor 5), also show moderate upward trends, suggesting that improvements in these areas can enhance perceived service quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative visualization of model performance (R²) across different modeling approaches.\n",
    "models_r2 = pd.DataFrame({\n",
    "    \"Model\": [\"Baseline Mean\", \"PCA Regression\", \"Factor Analysis Regression\"],\n",
    "    \"R²\": [0.10, 0.42, 0.64]\n",
    "})\n",
    "\n",
    "fig = px.bar(\n",
    "    models_r2, x=\"Model\", y=\"R²\", text=\"R²\",\n",
    "    title=\"Comparative Model Performance (R²)\",\n",
    "    color=\"Model\", template=\"plotly_white\"\n",
    ")\n",
    "fig.update_traces(texttemplate=\"%{text:.2f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Communalities visualization\n",
    "cols_used = [col for col in data_df.columns if col != 'satisfaction_proxy']\n",
    "\n",
    "communalities = fa.get_communalities()\n",
    "communalities_df = pd.DataFrame({\n",
    "    \"Variable\": cols_used[:len(communalities)],\n",
    "    \"Communality (h²)\": communalities\n",
    "})\n",
    "\n",
    "fig = px.bar(\n",
    "    communalities_df,\n",
    "    x=\"Variable\",\n",
    "    y=\"Communality (h²)\",\n",
    "    title=\"Communalities by Variable — Shared Variance Explained by Factors\",\n",
    "    text=\"Communality (h²)\",\n",
    "    color=\"Communality (h²)\",\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.update_traces(texttemplate=\"%{text:.2f}\", textposition=\"outside\")\n",
    "fig.update_layout(height=600, width=900, title_x=0.5, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The communality chart illustrates how well each observed variable is represented by the extracted factors.  \n",
    "Most items show communalities between *0.60 and 0.74*, indicating that the factor model explains a substantial portion of their variance.\n",
    "\n",
    "Variables such as technical_expertise, problem_solving, and innovation_solutions have the *highest communalities (~0.73–0.74)*, confirming that they are central indicators of the latent “Technical Excellence” factor.  \n",
    "\n",
    "On the other hand, items like change_management and value_for_money display *lower communalities (~0.47–0.51)*, suggesting they capture more specific or unique variance not fully shared with other constructs.  \n",
    "\n",
    "Overall, the communalities distribution supports the reliability of the five-factor solution, showing that the model adequately represents most key dimensions of customer satisfaction while leaving room for item-specific insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategic Prioritization Chart\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "strategy_df = pd.DataFrame({\n",
    "    \"Factor\": [\"Factor 1\", \"Factor 2\", \"Factor 3\", \"Factor 4\", \"Factor 5\"],\n",
    "    \"Impact (R²)\": [0.637, 0.026, 0.012, 0.034, 0.019],\n",
    "    \"Control (Ease of Improvement)\": [2, 3, 4, 3, 2],  \n",
    "    \"Business Dimension\": [\n",
    "        \"Technical Excellence & Innovation\",\n",
    "        \"Relationship Management & Trust\",\n",
    "        \"Financial Transparency & Value\",\n",
    "        \"Project Execution & Delivery\",\n",
    "        \"Customer Support & Service\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "fig = px.scatter(\n",
    "    strategy_df,\n",
    "    x=\"Control (Ease of Improvement)\",\n",
    "    y=\"Impact (R²)\",\n",
    "    text=\"Factor\",\n",
    "    color=\"Business Dimension\",\n",
    "    title=\"Strategic Prioritization Matrix — Impact vs. Control\",\n",
    "    size=\"Impact (R²)\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.update_traces(marker=dict(sizeref=2.*max(strategy_df['Impact (R²)'])/(100**2), sizemode='area'))\n",
    "fig.update_yaxes(range=[0, 0.7])\n",
    "fig.update_layout(height=600, width=900, title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategic Prioritization Matrix**\n",
    "\n",
    "The matrix plots each latent factor by its *impact on customer satisfaction (R²)* and its *ease of improvement (control)*.  \n",
    "Factor 1 — Technical Excellence & Innovation — stands out as the clear strategic priority, combining the highest impact (R² = 0.64) with a moderate control level, suggesting that investments in technical expertise, innovation, and integration would yield the greatest measurable returns.  \n",
    "\n",
    "Factors 2–5 exhibit much lower direct impact values (R² < 0.05), implying secondary influence areas.  \n",
    "These can be addressed selectively, focusing on process optimization and service responsiveness once the core technical drivers have been strengthened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Include explanatory text**\n",
    "\n",
    "The bar chart above summarizes the relative contribution of each latent factor in explaining customer satisfaction outcomes.\n",
    "Factor 1 (Technical Excellence & Innovation) clearly dominates the model with an R² of 0.637, confirming that customers’ perception of expertise, innovation, and technical reliability has the strongest measurable effect on overall satisfaction.\n",
    "\n",
    "Meanwhile, Factors 2, 4, and 5—related to Relationship Management, Project Delivery, and Customer Support—show smaller but still meaningful contributions, reflecting their supporting role in building long-term trust and post-delivery satisfaction.\n",
    "Factor 3 (Financial Transparency & Value) presents the lowest numerical influence, yet remains essential for maintaining fairness and perceived ROI.\n",
    "\n",
    "These findings emphasize that investments in technical quality, innovation, and delivery performance yield the highest business impact, while maintaining transparency and responsiveness reinforces customer retention and brand credibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Include explanatory text**\n",
    "\n",
    "The business impact summary consolidates how each latent factor influences customer satisfaction outcomes. According to the regression and correlation analysis, Factor 1 is the dominant driver of overall satisfaction, with an R² of 0.60, indicating that customers’ perception of expertise, innovation, and technical reliability most strongly determines their experience with TechnoServe Solutions.\n",
    "\n",
    "Factor 4 also contributes significantly, highlighting the importance of on-time delivery, quality assurance, and effective project management practices.\n",
    "\n",
    "Meanwhile, Factor 2 supports long-term partnerships by strengthening customer confidence and engagement. Though Factors 3 and 5 show smaller numerical coefficients, they remain essential for maintaining transparency and post-service quality, acting as reinforcing pillars of sustained satisfaction.\n",
    "\n",
    "Overall, the visualization and model evidence suggest that investments in technical innovation, robust delivery systems, and transparent communication yield the greatest measurable business impact, directly linking operational performance to client satisfaction and retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Information \n",
    " \n",
    "**Team: 5** [Girls] \n",
    " \n",
    "**Members:**\n",
    "- [Sibyla Vera Avila] ([01665122]) - Data exploration and factor extraction \n",
    "- [Sophia Gabriela Martínez Albarrán] ([A01424430]) - Factor interpretation and business insights \n",
    "- [Regina Pérez Vázquez] ([A01659356]) - Visualization and recommendations \n",
    " \n",
    "**Deliverable Links:**\n",
    "- **Presentation Video:** [YouTube Link] \n",
    "- **Executive Summary:** [Available on Canvas] \n",
    "- **Dataset:** `customer_satisfaction_data.csv` \n",
    " \n",
    "**Completion Date:** [2/11/2025]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
